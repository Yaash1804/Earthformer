# @package _global_

# --- Dataset Configuration ---
dataset:
  _target_: src.earthformer.datasets.sst.sst_datamodule.SSTDataModule
  # MODIFIED: Path updated to your new 'sst' data folder
  data_root: "datasets/sst"
  in_len: 12
  out_len: 12
  batch_size: 2 # Lowered for large data, adjust based on your GPU memory
  num_workers: 4
  # MODIFIED: Switched to end years for clearer date-based splitting
  train_end_year: 2015  # Training data will be up to the end of 2015
  val_end_year: 2020    # Validation data will be 2016-2020

# --- Model Configuration ---
model:
  _target_: src.earthformer.cuboid_transformer.CuboidTransformerModel
  in_len: ${dataset.in_len}
  out_len: ${dataset.out_len}
  in_channels: 1
  out_channels: 1
  # CRITICAL: Explicitly set the spatial dimensions of your data
  target_shape: 

  # --- Model Hyperparameters (can be tuned later) ---
  num_blocks: 1
  num_layers: 6
  num_global_vectors: 8
  dim: 512
  hidden_dim: 2048
  num_heads: 8
  encoder_depth: 3
  decoder_depth: 3
  downsample: 2
  use_global_vector: True
  use_autoregression: False
  
  loss: "mae"
  metrics: ["mse", "mae", "rmse"]
  
# --- Optimizer and Scheduler Configuration ---
optim:
  optimizer:
    _target_: torch.optim.AdamW
    lr: 1.0e-4
    betas: [0.9, 0.95]
    weight_decay: 0.05
  scheduler:
    _target_: src.earthformer.optim.schedulers.LinearWarmupCosineAnnealingLR
    warmup_epochs: 10
    max_epochs: ${trainer.max_epochs}
    warmup_start_lr: 1.0e-6
    eta_min: 1.0e-6

# --- Logging Configuration ---
logging:
  save_dir: "lightning_logs/sst"
  project: "earthformer"
  name: "cuboid_transformer_sst_singlefile"

# --- Trainer Configuration ---
trainer:
  gpus: 1
  strategy: "ddp"
  max_epochs: 50 # Start with fewer epochs to test the pipeline
  precision: 16 # Use 16-bit precision to save memory
  val_check_interval: 1.0
  log_every_n_steps: 10
  check_val_every_n_epoch: 1

seed: 2022